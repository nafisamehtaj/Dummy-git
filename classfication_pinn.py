# -*- coding: utf-8 -*-
"""Classfication-PINN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rYfwN_Rplc14LT2L_M3ykHCoTwRGjaL
"""

# import necessary libraries

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
import collections
import random
import math

# device and warnings

warnings.filterwarnings("ignore")
torch.set_default_dtype(torch.float64)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MULTI_GPU = torch.cuda.device_count() > 1
print(f"Running on {DEVICE} with {torch.cuda.device_count()} GPUs")

# random seed applying for stable initialization and avoiding randomness

def set_seed(seed=42):
    random.seed(seed)  # Python random module
    np.random.seed(seed)  # NumPy
    torch.manual_seed(seed)  # CPU
    torch.cuda.manual_seed(seed)  # GPU (if using CUDA)
    torch.cuda.manual_seed_all(seed)  # All GPU devices (if using multi-GPU)

    # Make deterministic (slower but reproducible)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Set the seed
set_seed(42)

# ensuring the output directory for storing results

# === Ensure output directory exists early ===
os.makedirs("outputs", exist_ok=True)

# load the input data

# Load data
data = pd.read_csv("/content/new_dataset_18_timesample(m).csv")

# seperating the input and output parameter from the data
X = data[['x', 'y', 't']].to_numpy()
y = data[['u', 'v']].to_numpy()

# converting the input and output numpy to tensor float 64
X = torch.tensor(X, dtype=torch.float64)
y = torch.tensor(y, dtype=torch.float64)

# necessaryf functions for preprocessing the training data

# checking on the numerical values of y
''' what is the output of this function?
showing that how many positive and negative value we have in
our displacement value. also, what are the power range'''

def plot_magnitude_distribution(y):
    magnitudes = torch.abs(y.flatten())
    zero_count = (magnitudes == 0).sum().item()

    y_flat = y.flatten()
    pos_count = (y_flat > 0).sum().item()
    neg_count = (y_flat < 0).sum().item()

    nonzero_mags = magnitudes[magnitudes > 0]
    log_mags = torch.log10(nonzero_mags)
    bins = torch.floor(log_mags).to(torch.int32)
    counter = collections.Counter(bins.tolist())

    print("Sign Distribution:")
    print(f"Positive values : {pos_count:>8}")
    print(f"Negative values : {neg_count:>8}")
    print(f"Exact Zeros     : {zero_count:>8}\n")

    print("Magnitude Level Distribution:")
    print(f"0     : {zero_count:>9} values (exact zeros)")
    for exp in sorted(counter.keys(), reverse=True):
        print(f"10^{exp:>2} : {counter[exp]:>9} values")

# Standardization formula:
'''x_standardized = (x - mean) / std
# where:
# mean = x.mean(0)
# std = x.std(0)
# 0 inside the std.() means column wise division
# if std == 0, set std = 1.0 to avoid division by zero'''

def standardize_tensor(x: torch.Tensor):

    x_mean = x.mean(0, keepdim=True)
    x_std_val = x.std(0, keepdim=True)
    x_std_val[x_std_val == 0] = 1.0
    x_std = (x - x_mean) / x_std_val
    return x_std

# preprocessing the training data

X_standerdized = standardize_tensor(X)
y_standerdized = standardize_tensor(y)

plot_magnitude_distribution(y_standerdized)

# function for time based train test split

def split_train_test_by_step(X: torch.Tensor, y: torch.Tensor, samples_per_step: int, test_step: int):
    total_steps = X.shape[0] // samples_per_step
    train_steps = [i for i in range(total_steps) if i != test_step]
    X_train = torch.cat([X[i * samples_per_step:(i + 1) * samples_per_step] for i in train_steps])
    y_train = torch.cat([y[i * samples_per_step:(i + 1) * samples_per_step] for i in train_steps])
    X_test = X[test_step * samples_per_step:(test_step + 1) * samples_per_step]
    y_test = y[test_step * samples_per_step:(test_step + 1) * samples_per_step]
    return X_train, y_train, X_test, y_test

# spliting the timestep based train test data

X_train, y_train, X_test, y_test = split_train_test_by_step(X_standerdized, y_standerdized, samples_per_step=60000, test_step=13)

# preparing batchwise train data to feed the NN
# Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.
# use chatgpt to see what's inside the dataloader

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=10000, shuffle=True)

#  Config for new head & losses
# -----------------------------
exp_max = 1   # exponent classes from 2 down to -9  (1, 0, -1, ..., -9) => 11 classes
exp_min = -9
exp_classes = list(range(exp_max, exp_min - 1, -1))   # [1,0,-1,...,-9]
num_exp_classes = len(exp_classes)                # 11

lambda_exp_class   = 1.0   # weight for exponent classification loss
lambda_significand  = 1.0   # weight for mantissa regression loss
lambda_data     = 1.0   # weight for reconstructed data loss
lambda_physics_data  = 1.0   # weight for phys-data consistency
# physics residual is combined as log(1 + lambda * phys_loss), same as before

#  Network
# ---------

class SinActivation(nn.Module):
    def forward(self, x): return torch.sin(x)

class TripleHeadNet(nn.Module):
    """
    Modified per request:
      - exponent head: classification over integer exponents from E_MAX .. E_MIN (size NUM_EXP_CLASSES) for each of (u,v)
      - mantissa head: regression (u,v) giving mantissas (can be signed; reconstruction uses mantissa * 10^(exp))
      - phys head: unchanged (produces [phi, zi])
    """
    def __init__(self, sizes):
        super().__init__()
        layers = []
        for i in range(len(sizes) - 1):
            layers.extend([nn.Linear(sizes[i], sizes[i + 1]), nn.ReLU()])
        self.backbone = nn.Sequential(*layers)

        # ----- exponent classification head -----
        # For u and v separately: output logits for 12 classes each => 2 * NUM_EXP_CLASSES
        self.exp_head = nn.Sequential(
            nn.Linear(sizes[-1], 64),
            SinActivation(),
            nn.Linear(64, 2 * num_exp_classes)  # logits; later reshape to (N, 2, NUM_EXP_CLASSES)
        )

        # ----- significand regression head -----
        self.significand_head = nn.Sequential(
            nn.Linear(sizes[-1], 32),
            SinActivation(),
            nn.Linear(32, 16),
            SinActivation(),
            nn.Linear(16, 2)  # (u_mant, v_mant) unconstrained (can be signed)
        )

        # ----- physics head (unchanged) -----
        self.phys_head = nn.Sequential(
            nn.Linear(sizes[-1], 32),
            SinActivation(),
            nn.Linear(32, 16),
            SinActivation(),
            nn.Linear(16, 2),
            SinActivation()
        )

        # PDE loss weight parameter (unchanged)
        self.raw_lambda = nn.Parameter(torch.tensor([0.0], dtype=torch.float64))

        # init
        self.apply(TripleHeadNet.he_init)

    @staticmethod
    def he_init(m):
        if isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    def get_lambda(self):
        return torch.sigmoid(self.raw_lambda)

    def forward(self, x):
        f = self.backbone(x)
        exp_logits = self.exp_head(f)                  # (N, 2*NUM_EXP_CLASSES)
        exp_logits = exp_logits.view(-1, 2, num_exp_classes)  # (N, 2, C)
        significand   = self.significand_head(f)             # (N, 2)
        phi_zi     = self.phys_head(f)                 # (N, 2)
        return exp_logits, significand, phi_zi

# physics

def compute_physics(phi_zi, X):
    phi, zi = phi_zi[:, 0], phi_zi[:, 1]
    grads_phi = torch.autograd.grad(phi, X, grad_outputs=torch.ones_like(phi), create_graph=True)[0]
    grads_zi  = torch.autograd.grad(zi, X, grad_outputs=torch.ones_like(zi),  create_graph=True)[0]
    d2phi_dx2 = torch.autograd.grad(grads_phi[:, 0], X, grad_outputs=torch.ones_like(grads_phi[:, 0]), create_graph=True)[0][:, 0]
    d2phi_dy2 = torch.autograd.grad(grads_phi[:, 1], X, grad_outputs=torch.ones_like(grads_phi[:, 1]), create_graph=True)[0][:, 1]
    d2phi_dt2 = torch.autograd.grad(grads_phi[:, 2], X, grad_outputs=torch.ones_like(grads_phi[:, 2]), create_graph=True)[0][:, 2]
    d2zi_dx2  = torch.autograd.grad(grads_zi[:, 0], X, grad_outputs=torch.ones_like(grads_zi[:, 0]), create_graph=True)[0][:, 0]
    d2zi_dy2  = torch.autograd.grad(grads_zi[:, 1], X, grad_outputs=torch.ones_like(grads_zi[:, 1]), create_graph=True)[0][:, 1]
    d2zi_dt2  = torch.autograd.grad(grads_zi[:, 2], X, grad_outputs=torch.ones_like(grads_zi[:, 2]), create_graph=True)[0][:, 2]
    cp2, cs2 = 6.3**2, 3.2**2
    f_phi = cp2 * (d2phi_dx2 + d2phi_dy2) - d2phi_dt2
    f_zi  = cs2 * (d2zi_dx2 + d2zi_dy2) - d2zi_dt2
    u_calc, v_calc = grads_phi[:, 0] + grads_zi[:, 1], grads_zi[:, 0] + grads_phi[:, 1]
    return f_phi, f_zi, u_calc, v_calc

#  Helpers for exponent/mantissa labeling
# ----------------------------------------

# converting the exponent classes values into tensor
exp_classes_tensor = torch.tensor(exp_classes, dtype=torch.float64, device=DEVICE)  # shape (C,)

def get_significand_and_exponent(x: torch.Tensor):
    eps = 1e-12
    x_abs = torch.clamp(torch.abs(x), min=eps)
    raw_exponents = torch.floor(torch.log10(x_abs))
    significands = x / torch.pow(10.0, raw_exponents)

    raw_exponents = torch.clamp(raw_exponents, min=exp_min, max=exp_max)
    exponent_indices = (exp_max - raw_exponents).to(torch.long)

    return significands, exponent_indices

def soft_average(probabilities: torch.Tensor, values: torch.Tensor):

    if probabilities.shape[-1] != values.shape[0]:
        raise ValueError("Mismatch: probabilities last dimension and values length must match.")

    return torch.sum(probabilities * values, dim=-1)

def reconstruct_from_heads(exp_logits, significand_pred):

    probs = F.softmax(exp_logits, dim=-1)
    e_hat = soft_average(probs, exp_classes_tensor)
    y_hat = significand_pred * torch.pow(10.0, e_hat) # will not consider for loss calculation
    return y_hat, e_hat, probs

#  Training step
# -----------------

def train(model, loader, optimizer, scheduler, epochs):
    model.train()

    total_loss, phys_loss, exp_loss, significand_loss = [], [], [], []
    log_vals, lambda_vals = [], []

    ce_loss = nn.CrossEntropyLoss()
    mse = nn.MSELoss()

    best_loss = float("inf")
    epochs_no_improve = 0
    best_state = None

    for epoch in range(epochs):
        epoch_total = epoch_data = epoch_phys = epoch_physdata = epoch_exp = epoch_significand = 0.0
        epoch_log = 0.0

        for Xb, Yb in loader:
            Xb, Yb = Xb.to(DEVICE), Yb.to(DEVICE)
            Xb.requires_grad_(True)

            # original labels for loss cal
            significand, exponent = get_significand_and_exponent(Yb)  # shapes (N,2),(N,2),(N,2)

            optimizer.zero_grad()

            # forward
            exp_logits, significand_pred, phi_zi = model(Xb)     # (N,2,C), (N,2), (N,2)

            # reconstruct y from heads (differentiable expected exponent)
            y_hat, e_hat, probs = reconstruct_from_heads(exp_logits, significand_pred) # (N,2)

            # physics
            f_phi, f_zi, u_calc, v_calc = compute_physics(phi_zi, Xb)

            # --- individual losses ---
            # exponent classification: reshape to (N*2, C) and targets to (N*2,)
            exp_logits_flat = exp_logits.reshape(-1, num_exp_classes)
            exp_targets_flat = exponent.reshape(-1)
            exp_cls_loss = ce_loss(exp_logits_flat, exp_targets_flat)

            # mantissa regression (MSE)
            significand_mse = mse(significand_pred, significand)

            # data loss on reconstructed y
            #data_loss = mse(preds, Yb)

            # physics-data consistency
            #phys_data_loss = F.smooth_l1_loss(u_calc, Yb[:, 0]) + F.smooth_l1_loss(v_calc, Yb[:, 1])

            # PDE residual
            phys_loss_val = F.smooth_l1_loss(f_phi, torch.zeros_like(f_phi)) + F.smooth_l1_loss(f_zi, torch.zeros_like(f_zi))

            # lambda (trainable)
            lam = model.module.get_lambda() if isinstance(model, nn.DataParallel) else model.get_lambda()

            lambda_exp_class   = 1.0   # weight for exponent classification loss
            lambda_significand  = 1.0   # weight for mantissa regression loss

            # total
            total = (lambda_exp_class * exp_cls_loss
                     + lambda_significand  * significand_mse
                     + torch.log(1 + lam * phys_loss_val))

            total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # accumulate
            epoch_total    += float(total.item())
            epoch_phys     += float(phys_loss_val.item())
            epoch_significand  += float(significand_mse.item())
            epoch_exp   += float(exp_cls_loss.item())
            epoch_log      += float(torch.log(1 + lam * phys_loss_val).item())

        total_loss.append(epoch_total)
        phys_loss.append(epoch_phys)
        significand_loss.append(epoch_significand)
        exp_loss.append(epoch_exp)
        lambda_vals.append(lam.item())
        log_vals.append(epoch_log)

        scheduler.step(epoch_total)
        print(f"Epoch {epoch+1:03d}: "
              f"Total={epoch_total:.3e}, "

              f"Phys={epoch_phys:.3e}, "

              f"Significand={epoch_significand:.3e}, "
              f"Exp={epoch_exp:.3e}")

        # === Early stopping check ===
        if epoch_total < best_loss - 1e-5:  # tiny tolerance to avoid floating point issues
            best_loss = epoch_total
            epochs_no_improve = 0
            best_state = model.state_dict()
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= 30:
            print(f"Early stopping at epoch {epoch+1} (no improvement in {30} epochs).")
            # restore best model
            if best_state is not None:
                model.load_state_dict(best_state)
            break

    return (total_loss, phys_loss, significand_loss, exp_loss,
            lambda_vals, log_vals)

model = TripleHeadNet([3, 64, 128, 128, 256, 256, 128])
if MULTI_GPU:
    model = nn.DataParallel(model)
model.to(DEVICE)

# optimizer

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5)

#  Train and save the model
# ------------
(out_total_losses,
 out_phys_losses,
 out_significand_losses,
 out_exp_losses,
 out_lambda_vals,
 out_log_vals) = train(model, train_loader, optimizer, scheduler, epochs=500)

# save the model
torch.save(model.state_dict(), "outputs/triple_head_model.pt")

def plot_losses(total_losses, phys_losses,  significand_loss, exp_loss, save_path='outputs/losses.png'):
    epochs = range(1, len(total_losses) + 1)

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, total_losses, label='Total Loss')
    plt.plot(epochs, phys_losses, label='Physics Loss')
    plt.plot(epochs, significand_loss, label='Significand Loss')
    plt.plot(epochs, exp_loss, label='Exponent MSE')

    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Components over Epochs')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


plot_losses(out_total_losses,
 out_phys_losses,
 out_significand_losses,
 out_exp_losses
 )

# explanatory example code
import math

x = 8.678e-0
exponent = math.floor(math.log10(abs(x)))
significand = x / (10 ** exponent)

print("Significand:", significand)
print("Exponent:", exponent)

# # explanatory example code

# def soft_average(probabilities, values):
#     """
#     Computes the expected value (soft average) given probabilities and values.

#     Args:
#         probabilities (list or numpy array): Probabilities (should sum to 1)
#         values (list or numpy array): Values associated with each probability

#     Returns:
#         float: The soft average (expected value)
#     """
#     if len(probabilities) != len(values):
#         raise ValueError("Length of probabilities and values must match.")

#     expected_value = sum(p * v for p, v in zip(probabilities, values))
#     return expected_value

# # explanatory example code

# probs = [0.7, 0.25, 0.05]
# exponents = [-9, -8, -7]

# result = soft_average(probs, exponents)
# print("Expected (soft average) exponent:", result)

def plot_uv_components_single_step(X: torch.Tensor, Y: torch.Tensor, model, filename='outputs/pred_vs_true_uv.png'):
    model.eval()
    X = X.to(DEVICE)

    with torch.no_grad():
        exp_logits, significand_pred, phi_zi = model(X)
        y_hat, e_hat, probs = reconstruct_from_heads(exp_logits, significand_pred)

    # De-standardize using ground-truth stats
    Y_mean = Y.mean(0, keepdim=True)
    Y_std = Y.std(0, keepdim=True)
    Y_std[Y_std == 0] = 1.0
    Y_pred = y_hat.cpu() * Y_std + Y_mean

    # Convert to numpy
    x_plot = X[:, 0].cpu().numpy()
    y_plot = X[:, 1].cpu().numpy()
    u_true = Y[:, 0].cpu().numpy()
    v_true = Y[:, 1].cpu().numpy()
    u_pred = Y_pred[:, 0].cpu().numpy()
    v_pred = Y_pred[:, 1].cpu().numpy()

    # Sort by grid
    sort_idx = np.lexsort((x_plot, y_plot))
    x_sorted, y_sorted = x_plot[sort_idx], y_plot[sort_idx]
    u_true_sorted, v_true_sorted = u_true[sort_idx], v_true[sort_idx]
    u_pred_sorted, v_pred_sorted = u_pred[sort_idx], v_pred[sort_idx]

    # Grid shape
    x_unique, y_unique = np.sort(np.unique(x_sorted)), np.sort(np.unique(y_sorted))
    nx, ny = len(x_unique), len(y_unique)

    x_grid = x_sorted.reshape(ny, nx)
    y_grid = y_sorted.reshape(ny, nx)
    u_true_grid = u_true_sorted.reshape(ny, nx)
    v_true_grid = v_true_sorted.reshape(ny, nx)
    u_pred_grid = u_pred_sorted.reshape(ny, nx)
    v_pred_grid = v_pred_sorted.reshape(ny, nx)

    # Plot
    fig, axs = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)
    plots = [
        (u_true_grid, 'Actual u'),
        (v_true_grid, 'Actual v'),
        (u_pred_grid, 'Predicted u'),
        (v_pred_grid, 'Predicted v')
    ]
    extent = [x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()]

    for ax, (data, label) in zip(axs.flatten(), plots):
        im = ax.imshow(data, extent=extent, origin='lower', cmap='viridis', aspect='auto')
        ax.set_title(label)
        ax.set_xlabel("x")
        ax.set_ylabel("y")
        plt.colorbar(im, ax=ax)

    plt.suptitle("Timestep-wise Displacement Prediction (u and v)", fontsize=16)
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.savefig(filename)
    plt.close()

plot_uv_components_single_step(X_test, y_test, model, filename='outputs/pred_vs_true_uv.png')

